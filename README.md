# 60 Days of Data Engineering

Inspired by **#100DaysOfCode**, I've decided to challenge myself into becoming a Data Engineer by studying and building Data/ML pipeline for 10-12 hours every day for the next 60 days. This started today 3rd of September and should be finished by 4th of November, 2019. My focus will be on ML/DL pipeline and Data Engineering tools around it such as KubeFlow, Apache Airflow, Apache Spark, Apache Kafka, and Tensorflow. I will document my progress on Github and update daily logs in LinkedIn.


## Day 1: September 3, 2019
Today's Progress: Spent time learning about Apache Airflow. Airflow is a platform to programmatically author, schedule and monitor workflows. Link: https://airflow.apache.org/index.html

Thoughts: Very happy with my progress, and excited to start building a Dynamodb to BigQuery ETL pipeline using Airflow tomorrow.

There are so many excellent blogs on Airflow. Today I want to share some beginner-friendly resources:

ðŸ‘‰ Airflow official documentation https://airflow.apache.org/index.html

ðŸ‘‰ Apache Airflow for the confused - Jonathan Pichot
https://medium.com/nyc-planning-digital/apache-airflow-for-the-confused-b588935669df

ðŸ‘‰ Apache Airflow: Tutorial and Beginners Guide
https://www.polidea.com/blog/apache-airflow-tutorial-and-beginners-guide/

ðŸ‘‰ Apache Airflow on Docker for Complete Beginners
https://medium.com/@itunpredictable/apache-airflow-on-docker-for-complete-beginners-cf76cf7b2c9a

ðŸ‘‰ Understanding Apache Airflowâ€™s key concepts
https://medium.com/@dustinstansbury/understanding-apache-airflows-key-concepts-a96efed52b1a

ðŸ‘‰ How to start automating your data pipelines with Airflow - Sriram Baskaran
https://blog.insightdatascience.com/airflow-101-start-automating-your-batch-workflows-with-ease-8e7d35387f94







