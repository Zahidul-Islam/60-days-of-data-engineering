# 60 Days of Data Engineering

Inspired by **#100DaysOfCode**, I've decided to challenge myself into becoming a Data Engineer by studying and building Data/ML pipeline for 10-12 hours every day for the next 60 days. This started today 3rd of September and should be finished by 4th of November, 2019. My focus will be on ML/DL pipeline and Data Engineering tools around it such as KubeFlow, Apache Airflow, Apache Spark, Apache Kafka, and Tensorflow. I will document my progress on Github and update daily logs in LinkedIn.

## Day 0: September 3, 2019 (template)
**Today's Progress:** 

**Thoughts:** 

**Link to work:**

## Day 1: September 3, 2019
**Today's Progress:** Spent time learning about Apache Airflow. Airflow is a platform to programmatically author, schedule and monitor workflows. Link: https://airflow.apache.org/index.html

**Thoughts:** Very happy with my progress, and excited to start building a Dynamodb to BigQuery ETL pipeline using Airflow tomorrow.

**There are so many excellent blogs on Airflow. Today I want to share some beginner-friendly resources:**

ðŸ‘‰ Airflow official documentation https://airflow.apache.org/index.html

ðŸ‘‰ Apache Airflow for the confused - Jonathan Pichot
https://medium.com/nyc-planning-digital/apache-airflow-for-the-confused-b588935669df

ðŸ‘‰ Apache Airflow: Tutorial and Beginners Guide
https://www.polidea.com/blog/apache-airflow-tutorial-and-beginners-guide/

ðŸ‘‰ Apache Airflow on Docker for Complete Beginners
https://medium.com/@itunpredictable/apache-airflow-on-docker-for-complete-beginners-cf76cf7b2c9a

ðŸ‘‰ Understanding Apache Airflowâ€™s key concepts
https://medium.com/@dustinstansbury/understanding-apache-airflows-key-concepts-a96efed52b1a

ðŸ‘‰ How to start automating your data pipelines with Airflow - Sriram Baskaran
https://blog.insightdatascience.com/airflow-101-start-automating-your-batch-workflows-with-ease-8e7d35387f94

## Day 2: September 4, 2019
**Today's Progress:** I wrote a blog post on LinkedIn where I explained Apache Airflow core concepts.  

**Thoughts:** There are so many interesting concepts in Airflow. It is an excellent tool for workflow orchestration. I want to spend more time on building custom **Operator**, **Hook** and data pipeline.

**Link to work:** [Apache Airflow Core Concepts](https://www.linkedin.com/pulse/apache-airflow-core-concepts-zahidul-islam/?trackingId=X3YNEn0IQHehblxk9G0Z7Q%3D%3D)

**Here are some useful links:**

ðŸ‘‰ A Definitive Compilation of Apache Airflow Resources - Aakash Pydi
https://towardsdatascience.com/a-definitive-compilation-of-apache-airflow-resources-82bc4980c154

ðŸ‘‰ DAG Writing Best Practices in Apache Airflow
https://www.astronomer.io/guides/dag-best-practices/

ðŸ‘‰ Automate AWS Tasks Thanks to Airflow Hooks - Arnaud
https://blog.sicara.com/automate-aws-tasks-boto3-airflow-hooks-593c3120e8fc

ðŸ‘‰ Getting started with Apache Airflow - Adnan Siddiqi
https://towardsdatascience.com/getting-started-with-apache-airflow-df1aa77d7b1b

ðŸ‘‰ Orchestration and DAG Design in Apache Airflow â€” Two Approaches
https://medium.com/hashmapinc/orchestration-and-dag-design-in-apache-airflow-two-approaches-35edd3eaf7c0

ðŸ‘‰ Apache Airflow Core Concepts - Zahidul Islam
https://www.linkedin.com/pulse/apache-airflow-core-concepts-zahidul-islam/?trackingId=X3YNEn0IQHehblxk9G0Z7Q%3D%3D






